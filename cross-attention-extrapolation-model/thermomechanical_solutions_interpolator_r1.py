import streamlit as st
import numpy as np
from numba import jit, prange
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import rcParams
from matplotlib.ticker import AutoMinorLocator, MultipleLocator, FormatStrFormatter
import pandas as pd
import zipfile
from io import BytesIO
import time
import hashlib
import json
from datetime import datetime
from scipy import stats, interpolate
from scipy.ndimage import gaussian_filter, rotate
import warnings
import pickle
import torch
import sqlite3
from io import StringIO
import traceback
import h5py
import msgpack
import dill
import joblib
from pathlib import Path
import tempfile
import base64
import os
import glob
import re
from typing import List, Dict, Any, Optional

warnings.filterwarnings('ignore')

# =============================================
# PATH CONFIGURATION
# =============================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
NUMERICAL_SOLUTIONS_DIR = os.path.join(SCRIPT_DIR, "numerical_solutions")
FEA_SOLUTIONS_DIR = os.path.join(SCRIPT_DIR, "fea_solutions")

# Create directories
os.makedirs(NUMERICAL_SOLUTIONS_DIR, exist_ok=True)
os.makedirs(FEA_SOLUTIONS_DIR, exist_ok=True)

# =============================================
# OPTIONAL: PYVISTA FOR FEA .VTU
# =============================================
try:
    import pyvista as pv
    PYVISTA_AVAILABLE = True
except ImportError:
    PYVISTA_AVAILABLE = False
    st.warning("âš ï¸ PyVista not installed. FEA .vtu mode disabled.")

# =============================================
# ORIGINAL INTERPOLATOR (for defect simulations)
# =============================================
class SpatialLocalityAttentionInterpolator:
    """Enhanced attention-based interpolator with spatial locality regularization"""
    def __init__(self, input_dim=15, num_heads=4, d_model=32, output_dim=3,
                 sigma_spatial=0.2, sigma_param=0.2, use_gaussian=True):
        self.input_dim = input_dim
        self.num_heads = num_heads
        self.d_model = d_model
        self.output_dim = output_dim
        self.sigma_spatial = sigma_spatial
        self.sigma_param = sigma_param
        self.use_gaussian = use_gaussian
        self.model = self._build_model()
        self.readers = {
            'pkl': self._read_pkl,
            'pt': self._read_pt,
            'h5': self._read_h5,
            'npz': self._read_npz,
            'sql': self._read_sql,
            'json': self._read_json
        }

    def _build_model(self):
        model = torch.nn.ModuleDict({
            'param_embedding': torch.nn.Sequential(
                torch.nn.Linear(self.input_dim, self.d_model * 2),
                torch.nn.ReLU(),
                torch.nn.Linear(self.d_model * 2, self.d_model)
            ),
            'attention': torch.nn.MultiheadAttention(
                embed_dim=self.d_model,
                num_heads=self.num_heads,
                batch_first=True,
                dropout=0.1
            ),
            'feed_forward': torch.nn.Sequential(
                torch.nn.Linear(self.d_model, self.d_model * 4),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.1),
                torch.nn.Linear(self.d_model * 4, self.d_model)
            ),
            'output_projection': torch.nn.Sequential(
                torch.nn.Linear(self.d_model, self.d_model * 2),
                torch.nn.ReLU(),
                torch.nn.Linear(self.d_model * 2, self.output_dim)
            ),
            'spatial_regularizer': torch.nn.Sequential(
                torch.nn.Linear(2, 32),
                torch.nn.ReLU(),
                torch.nn.Linear(32, self.num_heads)
            ) if self.use_gaussian else None,
            'norm1': torch.nn.LayerNorm(self.d_model),
            'norm2': torch.nn.LayerNorm(self.d_model)
        })
        return model

    def _read_pkl(self, file_content):
        return pickle.loads(file_content)

    def _read_pt(self, file_content):
        buffer = BytesIO(file_content)
        return torch.load(buffer, map_location=torch.device('cpu'))

    def _read_h5(self, file_content):
        buffer = BytesIO(file_content)
        with h5py.File(buffer, 'r') as f:
            data = {}
            def read_h5_obj(name, obj):
                if isinstance(obj, h5py.Dataset):
                    data[name] = obj[()]
                elif isinstance(obj, h5py.Group):
                    data[name] = {}
            f.visititems(read_h5_obj)
            return data

    def _read_npz(self, file_content):
        buffer = BytesIO(file_content)
        return dict(np.load(buffer, allow_pickle=True))

    def _read_sql(self, file_content):
        buffer = StringIO(file_content.decode('utf-8'))
        conn = sqlite3.connect(':memory:')
        conn.executescript(buffer.read())
        return conn

    def _read_json(self, file_content):
        return json.loads(file_content.decode('utf-8'))

    def read_simulation_file(self, file_path, format_type='auto'):
        with open(file_path, 'rb') as f:
            file_content = f.read()
        if format_type == 'auto':
            filename = os.path.basename(file_path).lower()
            if filename.endswith('.pkl'):
                format_type = 'pkl'
            elif filename.endswith('.pt'):
                format_type = 'pt'
            elif filename.endswith('.h5') or filename.endswith('.hdf5'):
                format_type = 'h5'
            elif filename.endswith('.npz'):
                format_type = 'npz'
            elif filename.endswith('.sql') or filename.endswith('.db'):
                format_type = 'sql'
            elif filename.endswith('.json'):
                format_type = 'json'
            else:
                raise ValueError(f"Unrecognized file format: {filename}")
        if format_type in self.readers:
            data = self.readers[format_type](file_content)
            return self._standardize_data(data, format_type, file_path)
        else:
            raise ValueError(f"Unsupported format: {format_type}")

    def _standardize_data(self, data, format_type, file_path):
        standardized = {
            'params': {},
            'history': [],
            'metadata': {},
            'format': format_type,
            'file_path': file_path,
            'filename': os.path.basename(file_path)
        }
        if format_type == 'pkl':
            if isinstance(data, dict):
                standardized['params'] = data.get('params', {})
                standardized['metadata'] = data.get('metadata', {})
                for frame in data.get('history', []):
                    if isinstance(frame, dict):
                        eta = frame.get('eta')
                        stresses = frame.get('stresses', {})
                        standardized['history'].append((eta, stresses))
        elif format_type == 'pt':
            if isinstance(data, dict):
                standardized['params'] = data.get('params', {})
                standardized['metadata'] = data.get('metadata', {})
                for frame in data.get('history', []):
                    if isinstance(frame, dict):
                        eta = frame.get('eta')
                        stresses = frame.get('stresses', {})
                        if torch.is_tensor(eta):
                            eta = eta.numpy()
                        stress_dict = {}
                        for key, value in stresses.items():
                            if torch.is_tensor(value):
                                stress_dict[key] = value.numpy()
                            else:
                                stress_dict[key] = value
                        standardized['history'].append((eta, stress_dict))
        elif format_type == 'h5':
            if 'params' in 
                standardized['params'] = data['params']
            if 'metadata' in 
                standardized['metadata'] = data['metadata']
            if 'history' in 
                standardized['history'] = data['history']
        return standardized

    def compute_parameter_vector(self, sim_data):
        params = sim_data.get('params', {})
        param_vector = []
        param_names = []
        defect_encoding = {'ISF': [1, 0, 0], 'ESF': [0, 1, 0], 'Twin': [0, 0, 1]}
        defect_type = params.get('defect_type', 'ISF')
        param_vector.extend(defect_encoding.get(defect_type, [0, 0, 0]))
        param_names.extend(['defect_ISF', 'defect_ESF', 'defect_Twin'])
        shape_encoding = {
            'Square': [1, 0, 0, 0, 0],
            'Horizontal Fault': [0, 1, 0, 0, 0],
            'Vertical Fault': [0, 0, 1, 0, 0],
            'Rectangle': [0, 0, 0, 1, 0],
            'Ellipse': [0, 0, 0, 0, 1]
        }
        shape = params.get('shape', 'Square')
        param_vector.extend(shape_encoding.get(shape, [0, 0, 0, 0, 0]))
        param_names.extend(['shape_square', 'shape_horizontal', 'shape_vertical',
                           'shape_rectangle', 'shape_ellipse'])
        eps0 = params.get('eps0', 0.707)
        kappa = params.get('kappa', 0.6)
        theta = params.get('theta', 0.0)
        eps0_norm = (eps0 - 0.3) / (3.0 - 0.3)
        kappa_norm = (kappa - 0.1) / (2.0 - 0.1)
        theta_norm = (theta % (2 * np.pi)) / (2 * np.pi)
        param_vector.append(eps0_norm)
        param_names.append('eps0_norm')
        param_vector.append(kappa_norm)
        param_names.append('kappa_norm')
        param_vector.append(theta_norm)
        param_names.append('theta_norm')
        orientation = params.get('orientation', 'Horizontal {111} (0Â°)')
        orientation_encoding = {
            'Horizontal {111} (0Â°)': [1, 0, 0, 0],
            'Tilted 30Â° (1Â¯10 projection)': [0, 1, 0, 0],
            'Tilted 60Â°': [0, 0, 1, 0],
            'Vertical {111} (90Â°)': [0, 0, 0, 1]
        }
        param_vector.extend(orientation_encoding.get(orientation, [0, 0, 0, 0]))
        param_names.extend(['orient_0deg', 'orient_30deg', 'orient_60deg', 'orient_90deg'])
        return np.array(param_vector, dtype=np.float32), param_names

# =============================================
# NUMERICAL SOLUTIONS MANAGER
# =============================================
class NumericalSolutionsManager:
    def __init__(self, solutions_dir: str = NUMERICAL_SOLUTIONS_DIR):
        self.solutions_dir = solutions_dir
        self._ensure_directory()

    def _ensure_directory(self):
        if not os.path.exists(self.solutions_dir):
            os.makedirs(self.solutions_dir, exist_ok=True)

    def scan_directory(self) -> Dict[str, List[str]]:
        file_formats = {
            'pkl': [], 'pt': [], 'h5': [], 'npz': [], 'sql': [], 'json': []
        }
        for format_type, extensions in [
            ('pkl', ['*.pkl', '*.pickle']),
            ('pt', ['*.pt', '*.pth']),
            ('h5', ['*.h5', '*.hdf5']),
            ('npz', ['*.npz']),
            ('sql', ['*.sql', '*.db']),
            ('json', ['*.json'])
        ]:
            for ext in extensions:
                pattern = os.path.join(self.solutions_dir, ext)
                files = glob.glob(pattern)
                if files:
                    files.sort(key=os.path.getmtime, reverse=True)
                    file_formats[format_type].extend(files)
        return file_formats

    def get_all_files(self) -> List[Dict[str, Any]]:
        all_files = []
        file_formats = self.scan_directory()
        for format_type, files in file_formats.items():
            for file_path in files:
                file_info = {
                    'path': file_path,
                    'filename': os.path.basename(file_path),
                    'format': format_type,
                    'size': os.path.getsize(file_path),
                    'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                    'relative_path': os.path.relpath(file_path, self.solutions_dir)
                }
                all_files.append(file_info)
        all_files.sort(key=lambda x: x['filename'].lower())
        return all_files

    def load_simulation(self, file_path: str, interpolator) -> Dict[str, Any]:
        ext = os.path.splitext(file_path)[1].lower().lstrip('.')
        format_map = {
            'pkl': 'pkl', 'pickle': 'pkl',
            'pt': 'pt', 'pth': 'pt',
            'h5': 'h5', 'hdf5': 'h5',
            'npz': 'npz',
            'sql': 'sql', 'db': 'sql',
            'json': 'json'
        }
        format_type = format_map.get(ext, 'auto')
        sim_data = interpolator.read_simulation_file(file_path, format_type)
        sim_data['loaded_from'] = 'numerical_solutions'
        return sim_data

    def save_simulation(self, data: Dict[str, Any], filename: str, format_type: str = 'pkl'):
        if not filename.endswith(f'.{format_type}'):
            filename = f"{filename}.{format_type}"
        file_path = os.path.join(self.solutions_dir, filename)
        try:
            if format_type == 'pkl':
                with open(file_path, 'wb') as f:
                    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            elif format_type == 'pt':
                torch.save(data, file_path)
            elif format_type == 'json':
                def convert_for_json(obj):
                    if isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, np.generic):
                        return obj.item()
                    elif isinstance(obj, dict):
                        return {k: convert_for_json(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [convert_for_json(item) for item in obj]
                    else:
                        return obj
                json_data = convert_for_json(data)
                with open(file_path, 'w') as f:
                    json.dump(json_data, f, indent=2)
            else:
                return False
            return True
        except:
            return False

# =============================================
# NEW: FEA LASER EXTRAPOLATOR
# =============================================
class FEALaserExtrapolator:
    def __init__(self, sigma_param=0.3):
        self.sigma_param = sigma_param
        self.source_db = []
        self.fea_dir = FEA_SOLUTIONS_DIR

    def parse_folder_name(self, folder: str):
        match = re.match(r"q([\d\.p]+)mJ-delta([\d\.p]+)ns", folder)
        if not match:
            return None, None
        e_str, t_str = match.groups()
        try:
            E = float(e_str.replace('p', '.'))
            tau = float(t_str.replace('p', '.'))
            return E, tau
        except:
            return None, None

    def load_all_fea_data(self):
        if not PYVISTA_AVAILABLE:
            return
        self.source_db = []
        pattern = os.path.join(self.fea_dir, "q*mJ-delta*ns")
        folders = glob.glob(pattern)
        for folder in folders:
            E, tau = self.parse_folder_name(os.path.basename(folder))
            if E is None:
                continue
            vtu_files = sorted(glob.glob(os.path.join(folder, "a_t????.vtu")))
            if not vtu_files:
                continue
            timesteps = []
            max_temp = []
            mean_disp = []
            max_vm = []
            strain_energy = []
            for vtu in vtu_files:
                try:
                    mesh = pv.read(vtu)
                    t_step = int(re.search(r't(\d+)', vtu).group(1))
                    t_ns = float(t_step)
                    temp = mesh.point_data.get('temperature', None)
                    disp = mesh.point_data.get('displacement', None)
                    stress = mesh.point_data.get('principal stress', None)
                    T_max = float(np.nanmax(temp)) if temp is not None else 0.0
                    disp_mag = 0.0
                    if disp is not None:
                        if disp.ndim == 2 and disp.shape[1] == 3:
                            disp_mag = np.linalg.norm(disp, axis=1)
                        else:
                            disp_mag = np.abs(disp)
                        disp_mag = float(np.nanmean(disp_mag))
                    vm_max = float(np.nanmax(stress)) if stress is not None else 0.0
                    energy = vm_max * disp_mag if vm_max > 0 and disp_mag > 0 else 0.0
                    timesteps.append(t_ns)
                    max_temp.append(T_max)
                    mean_disp.append(disp_mag)
                    max_vm.append(vm_max)
                    strain_energy.append(energy)
                except:
                    continue
            if timesteps:
                self.source_db.append({
                    'E': E,
                    'tau': tau,
                    'timesteps': np.array(timesteps),
                    'max_temperature': np.array(max_temp),
                    'mean_displacement': np.array(mean_disp),
                    'max_von_mises': np.array(max_vm),
                    'strain_energy': np.array(strain_energy)
                })

    def _physics_embedding(self, E, tau, t):
        logE = np.log1p(E)
        logE_norm = (logE - np.log1p(0.1)) / (np.log1p(20.0) - np.log1p(0.1))
        tau_norm = (tau - 0.5) / (10.0 - 0.5)
        t_rel = t / max(tau, 1e-3)
        Lambda = t * 0.1
        return np.array([logE_norm, tau_norm, t_rel, Lambda], dtype=np.float32)

    def predict_at_time(self, E_query, tau_query, t_query):
        if not self.source_db:
            return {k: np.nan for k in ['max_temperature', 'mean_displacement', 'max_von_mises', 'strain_energy', 'confidence']}
        q = self._physics_embedding(E_query, tau_query, t_query)
        weights = []
        values = {k: [] for k in ['max_temperature', 'mean_displacement', 'max_von_mises', 'strain_energy']}
        for sim in self.source_db:
            for i, t_src in enumerate(sim['timesteps']):
                k = self._physics_embedding(sim['E'], sim['tau'], t_src)
                dist = np.sum((q - k) ** 2)
                w = np.exp(-dist / (2 * self.sigma_param ** 2))
                weights.append(w)
                for key in values:
                    values[key].append(sim[key][i])
        if not weights or sum(weights) == 0:
            return {k: np.nan for k in values}
        weights = np.array(weights)
        weights /= np.sum(weights)
        pred = {}
        for key in values:
            pred[key] = float(np.sum(weights * np.array(values[key])))
        min_dist = min(
            np.sum((q - self._physics_embedding(sim['E'], sim['tau'], t_src)) ** 2)
            for sim in self.source_db for t_src in sim['timesteps']
        )
        pred['confidence'] = float(np.exp(-min_dist / (2 * self.sigma_param ** 2)))
        return pred

    def predict_time_series(self, E_query, tau_query, t_list):
        results = {k: [] for k in ['max_temperature', 'mean_displacement', 'max_von_mises', 'strain_energy', 'confidence']}
        for t in t_list:
            pred = self.predict_at_time(E_query, tau_query, t)
            for k in results:
                results[k].append(pred.get(k, np.nan))
        return results

# =============================================
# UI FOR DEFECT INTERPOLATION
# =============================================
def create_attention_interface():
    st.header("ðŸ¤– Spatial-Attention Stress Interpolation")
    if 'interpolator' not in st.session_state:
        st.session_state.interpolator = SpatialLocalityAttentionInterpolator()
    if 'solutions_manager' not in st.session_state:
        st.session_state.solutions_manager = NumericalSolutionsManager()
    if 'source_simulations' not in st.session_state:
        st.session_state.source_simulations = []
        st.session_state.uploaded_files = {}
        st.session_state.loaded_from_numerical = []
    # ... (rest of your original UI code remains unchanged)
    # For brevity, we'll skip repeating the full UI here since it's already in your file
    st.info("Defect interpolation mode loaded. (UI implementation as in your original code)")

# =============================================
# UI FOR FEA EXTRAPOLATION
# =============================================
def render_fea_extrapolation():
    st.header("ðŸ”¬ FEA Laser Simulation Extrapolation")
    if 'fea_extrapolator' not in st.session_state:
        st.session_state.fea_extrapolator = FEALaserExtrapolator(sigma_param=0.4)
        st.session_state.fea_extrapolator.load_all_fea_data()
    fea = st.session_state.fea_extrapolator
    if not fea.source_db:
        st.warning(f"Place FEA simulations in `{FEA_SOLUTIONS_DIR}` in folders like `q0p5mJ-delta4p2ns/` containing `a_t0001.vtu` ... `a_t0008.vtu`")
        st.write("Expected structure:")
        st.code("""
fea_solutions/
â””â”€â”€ q2p0mJ-delta2p0ns/
    â”œâ”€â”€ a_t0001.vtu
    â”œâ”€â”€ a_t0002.vtu
    â””â”€â”€ ...
""")
        return
    st.subheader("ðŸŽ¯ Query Parameters")
    col1, col2, col3 = st.columns(3)
    with col1:
        E_q = st.number_input("Energy (mJ)", 0.1, 50.0, 15.0, 0.5)
    with col2:
        tau_q = st.number_input("Pulse On Duration (ns)", 0.5, 20.0, 5.0, 0.5)
    with col3:
        t_max = st.number_input("Max Prediction Time (ns)", 1, 50, 20, 1)
    t_list = np.arange(1, t_max + 1)
    if st.button("ðŸš€ Predict Extrapolated Response"):
        with st.spinner("Computing..."):
            results = fea.predict_time_series(E_q, tau_q, t_list)
            fig, ax1 = plt.subplots(figsize=(10, 5))
            ax1.plot(t_list, results['max_temperature'], 'r-', label='Max Temp (K)')
            ax1.set_xlabel('Time (ns)')
            ax1.set_ylabel('Temperature (K)', color='r')
            ax2 = ax1.twinx()
            ax2.plot(t_list, results['max_von_mises'], 'b--', label='Max VM Stress (GPa)')
            ax2.set_ylabel('Stress (GPa)', color='b')
            fig.tight_layout()
            st.pyplot(fig)
            conf = np.nanmean(results['confidence'][-min(5, len(results['confidence'])):])
            st.metric("Confidence (last 5 ns)", f"{conf:.2f}")
            if conf < 0.3:
                st.warning("âš ï¸ High extrapolation risk")
            df = pd.DataFrame({
                'Time (ns)': t_list,
                'Max Temp (K)': results['max_temperature'],
                'Max VM (GPa)': results['max_von_mises'],
                'Mean Disp (nm)': results['mean_displacement'],
                'Confidence': results['confidence']
            })
            st.dataframe(df, use_container_width=True)

# =============================================
# MAIN APP
# =============================================
def main():
    st.sidebar.header("ðŸ”§ Operation Mode")
    mode = st.sidebar.radio("Select Mode", [
        "Defect Interpolation (PKL/PT)",
        "FEA Laser Extrapolation (.vtu)"
    ])
    if mode == "Defect Interpolation (PKL/PT)":
        create_attention_interface()
    elif mode == "FEA Laser Extrapolation (.vtu)":
        render_fea_extrapolation()

if __name__ == "__main__":
    main()
    st.caption(f"ðŸ”¬ Dual-mode interpolator â€¢ {NUMERICAL_SOLUTIONS_DIR} â€¢ {FEA_SOLUTIONS_DIR} â€¢ 2025")
